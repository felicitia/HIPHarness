{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.2334041595458984 seconds ---\n",
      "Assumption: no cache staleness\n",
      "Processed 38025 lines in /Users/felicitia/Desktop/CERT_Data/TopIP/708777107_sorted.csv\n",
      "#url in both training set and testing set / #url in testing set =  0.7274229074889867\n",
      "window size: 3 weight threshold: 0.8 start prefetch: 0.8\n",
      "prefetch num =  2880\n",
      "hit num =  3342\n",
      "cache size =  2880\n",
      "Precision =  0.4652777777777778\n",
      "Recall =  0.43944773175542406\n"
     ]
    }
   ],
   "source": [
    "# Dependency Graph (DG) based prefetching technique\n",
    "# Paper: Using Predictive Prefetching to Improve World Wide Web Latency, 1996\n",
    "# Assumption: no cache staleness \n",
    "# Need to computer URL = host + uri (might not be the actual URL b/c some uris already have host and some don't, \n",
    "# but it doesn't affect the analysis results b/c the format is the same, e.g., the uri that starts with host name\n",
    "# always starts with host name)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time as sys_time\n",
    "\n",
    "window_size = 3\n",
    "weight_threshold = 0.8\n",
    "start_prefetch = 30420\n",
    "log_file = '/Users/felicitia/Desktop/XXX/TopIP/708777107_sorted.csv'\n",
    "srcIP_idx = 0\n",
    "desIP_idx = 1\n",
    "host_idx = 2\n",
    "url_idx = 3\n",
    "time_idx = 4\n",
    "\n",
    "DG = nx.DiGraph()\n",
    "url_buffer = []\n",
    "cache = set()\n",
    "hit_set = set()\n",
    "prefetch_num = 0\n",
    "hit_num = 0\n",
    "training_set = set()\n",
    "testing_set = set()\n",
    "\n",
    "def updateDG(url_buffer):\n",
    "    global DG\n",
    "    head_node = url_buffer[0]\n",
    "    successors = set()\n",
    "    i = 1\n",
    "    while i < len(url_buffer):\n",
    "        if url_buffer[i] != head_node:\n",
    "            successors.add(url_buffer[i])\n",
    "        i += 1\n",
    "    for suc in successors:\n",
    "        if DG.has_edge(head_node, suc):\n",
    "            old_weight = DG[head_node][suc]['weight']\n",
    "            head_count = DG.nodes[head_node]['count']\n",
    "            new_weight = (old_weight*head_count + 1) / (head_count + 1)\n",
    "            DG[head_node][suc]['weight'] = new_weight\n",
    "        else: # edge doesn't exist\n",
    "            if DG.has_node(head_node):\n",
    "                DG.add_edge(head_node, suc)\n",
    "                # head_node has outgoing edges\n",
    "                if 'count' in DG.nodes[head_node]: \n",
    "                    DG[head_node][suc]['weight'] = 1 / (DG.nodes[head_node]['count'] + 1)\n",
    "                else:\n",
    "                    DG[head_node][suc]['weight'] = 1 # initial weight for new edge (old_weight*head_count + 1) / (head_count + 1)\n",
    "                    DG.nodes[head_node]['count'] = 0 # initial count for new node\n",
    "            else:\n",
    "                DG.add_edge(head_node, suc)\n",
    "                DG[head_node][suc]['weight'] = 1 # initial weight for new edge (old_weight*head_count + 1) / (head_count + 1)\n",
    "                DG.nodes[head_node]['count'] = 0 # initial count for new node\n",
    "                \n",
    "    # when the whole url_buffer has the same url, then head_node might not have 'count' attribute\n",
    "    if not DG.has_node(head_node):\n",
    "        DG.add_node(head_node)\n",
    "    if len(successors) == 0 and 'count' not in DG.nodes[head_node]:\n",
    "        DG.nodes[head_node]['count'] = 0\n",
    "        \n",
    "    DG.nodes[head_node]['count'] += 1\n",
    "    return\n",
    "\n",
    "def drawGraph(dg):\n",
    "    pos = nx.spring_layout(dg)\n",
    "    nx.draw_networkx(dg, pos, with_lables=True)\n",
    "    # node_labels = nx.get_node_attributes(dg,'count')\n",
    "    # nx.draw_networkx_labels(dg, pos, labels = node_labels)\n",
    "    edge_labels = nx.get_edge_attributes(dg,'weight')\n",
    "    nx.draw_networkx_edge_labels(dg, pos, labels = edge_labels)\n",
    "    plt.savefig(\"dg.png\", format=\"PNG\")\n",
    "    plt.show()\n",
    "\n",
    "def prefetch(dg, url, cache):\n",
    "    global weight_threshold\n",
    "    global prefetch_num\n",
    "    if dg.has_node(url):\n",
    "        for suc in dg.successors(url):\n",
    "            if dg[url][suc]['weight'] > weight_threshold and (suc not in cache):\n",
    "                prefetch_num += 1\n",
    "                cache.add(suc)\n",
    "                    \n",
    "start_time = sys_time.time()\n",
    "# building predictive model dynamically, start prefetching after url_buffer is full    \n",
    "with open(log_file) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        actual_url = row[host_idx] + row[url_idx]\n",
    "        if line_count <= start_prefetch:\n",
    "            training_set.add(actual_url)\n",
    "        else:\n",
    "            testing_set.add(actual_url)\n",
    "        # add elements to fill the buffer\n",
    "        if line_count < window_size:\n",
    "            url_buffer.append(actual_url)\n",
    "            if line_count == window_size - 1:\n",
    "                #print('url buffer: ', url_buffer, line_count)\n",
    "                updateDG(url_buffer)\n",
    "        \n",
    "        # update the buffer and prefetching\n",
    "        else:\n",
    "            current_url = actual_url\n",
    "            url_buffer.pop(0)\n",
    "            url_buffer.append(current_url)\n",
    "            #print('url buffer: ', url_buffer, line_count)\n",
    "            updateDG(url_buffer)\n",
    "            if line_count > start_prefetch:\n",
    "                if current_url in cache:\n",
    "                    hit_set.add(current_url)\n",
    "                    hit_num += 1\n",
    "                prefetch(DG, current_url, cache)\n",
    "        line_count += 1\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (sys_time.time() - start_time))\n",
    "    print('Assumption: no cache staleness')\n",
    "    print(f'Processed {line_count} lines in {log_file}')\n",
    "    print('#url in both training set and testing set / #url in testing set = ', \n",
    "          len(training_set.intersection(testing_set)) / len(testing_set))\n",
    "    print('window size:', window_size, 'weight threshold:', weight_threshold, 'start prefetch:', start_prefetch / line_count)\n",
    "    print('prefetch num = ', prefetch_num)\n",
    "    print('hit num = ', hit_num)\n",
    "    print('cache size = ', len(cache))\n",
    "    print('Precision = ', len(hit_set) / len(cache))\n",
    "    print('Recall = ', hit_num / (line_count-start_prefetch))\n",
    "    \n",
    "#    drawGraph(DG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.066741943359375 seconds ---\n",
      "Assumption: cache is used once\n",
      "Processed 38025 lines in /Users/felicitia/Desktop/CERT_Data/TopIP/708777107_sorted.csv\n",
      "#url in both training set and testing set / #url in testing set =  0.7274229074889867\n",
      "window size: 3 weight threshold: 0.8 start prefetch: 0.8\n",
      "prefetch num =  4603\n",
      "hit num =  2321\n",
      "cache size =  2282\n",
      "Precision =  0.5042363675863567\n",
      "Recall =  0.3051939513477975\n"
     ]
    }
   ],
   "source": [
    "# Dependency Graph (DG) based prefetching technique\n",
    "# Paper: Using Predictive Prefetching to Improve World Wide Web Latency, 1996\n",
    "# Assumption: cache is only used once\n",
    "# Need to computer URL = host + uri (might not be the actual URL b/c some uris already have host and some don't, \n",
    "# but it doesn't affect the analysis results b/c the format is the same, e.g., the uri that starts with host name\n",
    "# always starts with host name)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time as sys_time\n",
    "\n",
    "\n",
    "window_size = 3\n",
    "weight_threshold = 0.8\n",
    "start_prefetch = 30420\n",
    "log_file = '/Users/felicitia/Desktop/XXX/TopIP/708777107_sorted.csv'\n",
    "srcIP_idx = 0\n",
    "desIP_idx = 1\n",
    "host_idx = 2\n",
    "url_idx = 3\n",
    "time_idx = 4\n",
    "\n",
    "DG = nx.DiGraph()\n",
    "url_buffer = []\n",
    "cache = set()\n",
    "prefetch_num = 0\n",
    "hit_num = 0\n",
    "training_set = set()\n",
    "testing_set = set()\n",
    "\n",
    "def updateDG(url_buffer):\n",
    "    global DG\n",
    "    head_node = url_buffer[0]\n",
    "    successors = set()\n",
    "    i = 1\n",
    "    while i < len(url_buffer):\n",
    "        if url_buffer[i] != head_node:\n",
    "            successors.add(url_buffer[i])\n",
    "        i += 1\n",
    "    for suc in successors:\n",
    "        if DG.has_edge(head_node, suc):\n",
    "            old_weight = DG[head_node][suc]['weight']\n",
    "            head_count = DG.nodes[head_node]['count']\n",
    "            new_weight = (old_weight*head_count + 1) / (head_count + 1)\n",
    "            DG[head_node][suc]['weight'] = new_weight\n",
    "        else: # edge doesn't exist\n",
    "            if DG.has_node(head_node):\n",
    "                DG.add_edge(head_node, suc)\n",
    "                # head_node has outgoing edges\n",
    "                if 'count' in DG.nodes[head_node]: \n",
    "                    DG[head_node][suc]['weight'] = 1 / (DG.nodes[head_node]['count'] + 1)\n",
    "                else:\n",
    "                    DG[head_node][suc]['weight'] = 1 # initial weight for new edge (old_weight*head_count + 1) / (head_count + 1)\n",
    "                    DG.nodes[head_node]['count'] = 0 # initial count for new node\n",
    "            else:\n",
    "                DG.add_edge(head_node, suc)\n",
    "                DG[head_node][suc]['weight'] = 1 # initial weight for new edge (old_weight*head_count + 1) / (head_count + 1)\n",
    "                DG.nodes[head_node]['count'] = 0 # initial count for new node\n",
    "                \n",
    "    # when the whole url_buffer has the same url, then head_node might not have 'count' attribute\n",
    "    if not DG.has_node(head_node):\n",
    "        DG.add_node(head_node)\n",
    "    if len(successors) == 0 and 'count' not in DG.nodes[head_node]:\n",
    "        DG.nodes[head_node]['count'] = 0\n",
    "        \n",
    "    DG.nodes[head_node]['count'] += 1\n",
    "    return\n",
    "\n",
    "def drawGraph(dg):\n",
    "    pos = nx.spring_layout(dg)\n",
    "    nx.draw_networkx(dg, pos, with_lables=True)\n",
    "    # node_labels = nx.get_node_attributes(dg,'count')\n",
    "    # nx.draw_networkx_labels(dg, pos, labels = node_labels)\n",
    "    edge_labels = nx.get_edge_attributes(dg,'weight')\n",
    "    nx.draw_networkx_edge_labels(dg, pos, labels = edge_labels)\n",
    "    plt.savefig(\"dg.png\", format=\"PNG\")\n",
    "    plt.show()\n",
    "\n",
    "def prefetch(dg, url, cache):\n",
    "    global weight_threshold\n",
    "    global prefetch_num\n",
    "    if dg.has_node(url):\n",
    "        for suc in dg.successors(url):\n",
    "            if dg[url][suc]['weight'] > weight_threshold and (suc not in cache):\n",
    "                prefetch_num += 1\n",
    "                cache.add(suc)\n",
    "                    \n",
    "start_time = sys_time.time()\n",
    "# building predictive model dynamically, start prefetching after url_buffer is full    \n",
    "with open(log_file) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        actual_url = row[host_idx] + row[url_idx]\n",
    "        if line_count <= start_prefetch:\n",
    "            training_set.add(actual_url)\n",
    "        else:\n",
    "            testing_set.add(actual_url)\n",
    "        # add elements to fill the buffer\n",
    "        if line_count < window_size:\n",
    "            url_buffer.append(actual_url)\n",
    "            if line_count == window_size - 1:\n",
    "                #print('url buffer: ', url_buffer, line_count)\n",
    "                updateDG(url_buffer)\n",
    "        \n",
    "        # update the buffer and prefetching\n",
    "        else:\n",
    "            current_url = actual_url\n",
    "            url_buffer.pop(0)\n",
    "            url_buffer.append(current_url)\n",
    "            #print('url buffer: ', url_buffer, line_count)\n",
    "            updateDG(url_buffer)\n",
    "            if line_count > start_prefetch:\n",
    "                if current_url in cache:\n",
    "                    hit_num += 1\n",
    "                    cache.remove(current_url) # delete cache after used\n",
    "                prefetch(DG, current_url, cache)\n",
    "        line_count += 1\n",
    "        \n",
    "    print(\"--- %s seconds ---\" % (sys_time.time() - start_time))\n",
    "    print('Assumption: cache is used once')    \n",
    "    print(f'Processed {line_count} lines in {log_file}')\n",
    "    print('#url in both training set and testing set / #url in testing set = ', \n",
    "          len(training_set.intersection(testing_set)) / len(testing_set))\n",
    "    print('window size:', window_size, 'weight threshold:', weight_threshold, 'start prefetch:', start_prefetch / line_count)\n",
    "    print('prefetch num = ', prefetch_num)\n",
    "    print('hit num = ', hit_num)\n",
    "    print('cache size = ', len(cache))\n",
    "    print('Precision = ', hit_num / prefetch_num)\n",
    "    print('Recall = ', hit_num / (line_count-start_prefetch))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
